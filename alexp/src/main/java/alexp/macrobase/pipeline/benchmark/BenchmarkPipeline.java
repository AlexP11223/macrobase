package alexp.macrobase.pipeline.benchmark;

import alexp.macrobase.evaluation.GridSearch;
import alexp.macrobase.evaluation.memory.BasicMemoryProfiler;
import alexp.macrobase.explanation.Explanation;
import alexp.macrobase.outlier.Trainable;
import alexp.macrobase.outlier.Updatable;
import alexp.macrobase.pipeline.Pipeline;
import alexp.macrobase.pipeline.Pipelines;
import alexp.macrobase.pipeline.benchmark.config.*;
import alexp.macrobase.pipeline.benchmark.result.ExecutionResult;
import alexp.macrobase.pipeline.benchmark.result.ResultFileWriter;
import alexp.macrobase.pipeline.benchmark.result.ResultHolder;
import alexp.macrobase.pipeline.benchmark.result.ResultWriter;
import alexp.macrobase.pipeline.config.StringObjectMap;
import alexp.macrobase.streaming.StreamGenerator;
import alexp.macrobase.streaming.Windows.WindowManager;
import alexp.macrobase.utils.BenchmarkUtils;
import com.google.common.base.Strings;
import com.google.common.collect.Iterables;
import edu.stanford.futuredata.macrobase.analysis.classify.Classifier;
import edu.stanford.futuredata.macrobase.datamodel.DataFrame;
import edu.stanford.futuredata.macrobase.datamodel.Schema;
import org.apache.commons.io.FilenameUtils;

import java.util.*;
import java.util.function.Function;
import java.util.stream.Collectors;

import static alexp.macrobase.utils.BenchmarkUtils.aucCurve;

public class BenchmarkPipeline extends Pipeline {

    private final ExecutionType executionType;

    private final ExecutionConfig conf;
    private final String rootDataDir;
    private ResultWriter resultWriter;
    private final String timeColumn = "__autogenerated_time";
    private DataFrame dataFrame;
    private int[] labels;

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf) {
        this(executionType, conf, null, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir) {
        this(executionType, conf, rootDataDir, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir, ResultWriter resultWriter) {
        this.executionType = executionType;
        this.conf = conf;
        this.rootDataDir = rootDataDir;
        this.resultWriter = resultWriter;
    }

    public void run() throws Exception {
        if (resultWriter == null) {
            setupResultWriter();
        }

        switch (executionType) {
            case BATCH_CLASSIFICATION:
                classificationMode();
                break;
            case STREAMING_CLASSIFICATION:
                streamingMode();
                break;
            case EXPLANATION:
                explanationMode();
                break;
        }
    }

    private void classificationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();

        printInfo(String.format("Running %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);
        StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf);

        if (!algorithmParameters.equals(classifierConf.getParameters())) {
            out.println(algorithmParameters);
        }
        Classifier classifier = Pipelines.getClassifier(
                classifierConf.getAlgorithmId(),
                algorithmParameters,
                conf.getDatasetConfig().getMetricColumns(),
                conf.getDatasetConfig().getDatasetId());
        ResultHolder resultHolder = runClassifier(classifier);

        printInfo(String.format("Training time: %d ms (%.2f sec), Classification time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
                resultHolder.getTrainingTime(), resultHolder.getTrainingTime() / 1000.0,
                resultHolder.getClassificationTime(), resultHolder.getClassificationTime() / 1000.0,
                resultHolder.getMaxMemoryUsage() / 1024 / 1024,
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).rocArea()),
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).prArea())));

        resultWriter.write(resultHolder.getResultsDf(),
                new ExecutionResult(resultHolder.getTrainingTime(), resultHolder.getClassificationTime(),
                        0,
                        resultHolder.getMaxMemoryUsage(), conf, algorithmParameters)
                        .setClassifierId(classifierConf.getAlgorithmId())
        );
    }

    private void streamingMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //
        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
        StreamGenerator.init();
        final List<Long> streamTrainTime = new ArrayList<>();
        final List<Long> streamPredictTime = new ArrayList<>();
        final List<Long> streamUpdateTime = new ArrayList<>();
        String rawDataPoint = "";
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //
        WindowManager wm = new WindowManager(classifierConf, conf.getDatasetConfig()); // Initialize window manager
        String windowType = wm.getWindowMethod();
        if (windowType.equals("none")) { // Make sure that the current classifier is streaming classifier.
            return;
        }
        printInfo(String.format("Running %s %s on %s", classifierConf.getAlgorithmId(), classifierConf.getParameters(), conf.getDatasetConfig().getUri().getOriginalString()));
        if (resultWriter == null) { // Print the classifier information
            setupResultWriter();
        }
        StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf); // Validate the algorithm parameters
        if (!algorithmParameters.equals(classifierConf.getParameters())) {
            out.println(algorithmParameters);
        }
        Classifier streamingClassifier = Pipelines.getClassifier( // Build the Streaming Classifier Model
                classifierConf.getAlgorithmId(),
                algorithmParameters,
                conf.getDatasetConfig().getMetricColumns(),
                conf.getDatasetConfig().getDatasetId()
        );
        while (true) { // Iteratively Repeat (Streaming Simulation)
            if (!wm.windowIsConstructed()) {
                rawDataPoint = StreamGenerator.fetch(conf.getDatasetConfig().getUri().getPath()); // Read a raw data point from the Stream Generator
                wm.manage(rawDataPoint); // Obtain the window when the window method invariants are satisfied
                if (wm.getWindowSize() <= 0) {
                    break; // Stop streaming simulation when the real size of the window is zero
                }
            } else {
                dataFrame = wm.getWindowDF(); // Build the window DataFrame
                createAutoGeneratedColumns(dataFrame, timeColumn); // Add a time column to the DataFrame

                streamTrainTime.add(streamingClassifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
                    ((Trainable) streamingClassifier).train(dataFrame);
                }) : 0);

                streamPredictTime.add(BenchmarkUtils.measureTime(() -> {
                    streamingClassifier.process(dataFrame);
                }));

                streamUpdateTime.add(streamingClassifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
                    ((Updatable) streamingClassifier).update(dataFrame);
                }) : 0);

                wm.clearWindowData();  // Clear the window data (in order to continue to the next window construction)
                if (wm.isEndStream()) {
                    break; // Stop streaming simulation when the generator is empty
                }
            }
        }
        long streamMemoryUsage = memoryProfiler.getPeakUsage(); // total memory usage (per algorithm)
        DataFrame streamDF = streamingClassifier.getResults();
        if (streamDF != null) {
            // - - - - - - - - - - - - - - - - - - - /
            double[] scores = streamDF.getDoubleColumnByName(streamingClassifier.getOutputColumnName());
            int[] labels = getLabels(streamDF);
            printInfo(String.format("" +
                            "Training time: %f sec, " +
                            "Classification time: %f sec, " +
                            "Update time: %f sec, " +
                            "Max memory usage: %d MB, " +
                            "PR AUC: %s, " +
                            "ROC AUC: %s",
                    ((double) avg(streamTrainTime) / 1000.0),
                    ((double) avg(streamPredictTime) / 1000.0),
                    ((double) avg(streamUpdateTime) / 1000.0),
                    (streamMemoryUsage / 1024 / 1024),
                    String.format("%.4f", aucCurve(scores, labels).prArea()),
                    String.format("%.4f", aucCurve(scores, labels).rocArea())
            ));
            resultWriter.write(streamDF, new ExecutionResult(avg(streamTrainTime), avg(streamPredictTime), avg(streamUpdateTime),streamMemoryUsage, conf, algorithmParameters));
        } else {
            System.out.println("[Alert] There are no data points processed by the current algorithm");
        }
        // streaming mode has been completed.
    }

    private static long avg(List<Long> list) {
        long sum = 0;
        for (long i : list) {
            sum += i;
        }
        return sum / list.size();
    }

    private void explanationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();
        AlgorithmConfig explainerConf = conf.getExplainerConfig();

        printInfo(String.format("Running Classifier %s %s + Explainer %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                explainerConf.getAlgorithmId(), explainerConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);

        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();

        Explanation explainer = Pipelines.getExplainer(explainerConf, classifierConf, conf.getDatasetConfig(), conf.getSettingsConfig().getExplanationSettings());
        final long explanationTime = BenchmarkUtils.measureTime(() -> {
            explainer.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        System.out.println("\nTime " + explanationTime / 1000.0 + " sec");
//                printInfo(String.format("Explanation time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
//                        explanationTime, explanationTime / 1000.0,
//                        maxMemoryUsage / 1024 / 1024,
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).rocArea()),
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).prArea())));
        resultWriter.write(explainer.getResults(), new ExecutionResult(0, explanationTime, 0, maxMemoryUsage,
                conf, explainerConf.getParameters())
                .setClassifierId(classifierConf.getAlgorithmId())
                .setExplainerId(explainerConf.getAlgorithmId()));
    }

    private ResultHolder runClassifier(Classifier classifier) throws Exception {
        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
        final long trainingTime = classifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
            ((Trainable) classifier).train(dataFrame);
        }) : 0;
        final long classificationTime = BenchmarkUtils.measureTime(() -> {
            classifier.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        DataFrame resultsDf = classifier.getResults();
        return new ResultHolder(resultsDf, trainingTime, classificationTime, maxMemoryUsage);
    }

    private StringObjectMap getAlgorithmParameters(AlgorithmConfig algorithmConfig) throws Exception {
        StringObjectMap baseParams = algorithmConfig.getParameters();
        GridSearchConfig gridSearchConfig = algorithmConfig.getGridSearchConfig();
        if (gridSearchConfig == null) {
            return baseParams;
        }
        out.println(String.format("Running Grid Search, using %s measure", gridSearchConfig.getMeasure().toUpperCase()));
        GridSearch gs = new GridSearch();
        gs.addParams(gridSearchConfig.getParameters());
        gs.setOutputStream(out);
        gs.run(params -> {
            StringObjectMap currentParams = baseParams.merge(params);
            Classifier classifier = Pipelines.getClassifier(
                    algorithmConfig.getAlgorithmId(),
                    currentParams,
                    conf.getDatasetConfig().getMetricColumns(),
                    conf.getDatasetConfig().getDatasetId());
            classifier.process(dataFrame);
            DataFrame classifierResultDf = classifier.getResults();
            double[] classifierResult = classifierResultDf.getDoubleColumnByName(classifier.getOutputColumnName());
            switch (gridSearchConfig.getMeasure()) {
                case "roc":
                    return aucCurve(classifierResult, labels).rocArea();
                case "pr":
                    return aucCurve(classifierResult, labels).prArea();
                default:
                    throw new RuntimeException("Unknown search measure " + gridSearchConfig.getMeasure());
            }
        });
        SortedMap<Double, Map<String, Object>> gsResults = gs.getResults();
        return baseParams.merge(new StringObjectMap(Iterables.getLast(gsResults.values())));
    }

    private void setupResultWriter() {
        resultWriter = new ResultFileWriter(executionType)
                .setOutputDir(getOutputDir())
                .setBaseFileName(FilenameUtils.removeExtension(conf.getDatasetConfig().getDatasetId())); // doesn't matter, currently always initialized in constructor
    }

    private DataFrame loadData() throws Exception {
        Map<String, Schema.ColType> colTypes = getColTypes();

        List<String> requiredColumns = new ArrayList<>(colTypes.keySet());

        DataFrame dataFrame = Pipelines.loadDataFrame(conf.getDatasetConfig().getUri().addRootPath(rootDataDir), colTypes, requiredColumns, conf.getDatasetConfig().toMap());

        createAutoGeneratedColumns(dataFrame, timeColumn); // needed for MCOD

        return dataFrame;
    }

    private int[] getLabels(DataFrame dataFrame) {
        String labelColumn = conf.getDatasetConfig().getLabelColumn();
        if (Strings.isNullOrEmpty(labelColumn)) {
            return null;
        }

        return Arrays.stream(dataFrame.getDoubleColumnByName(labelColumn))
                .mapToInt(d -> (int) d)
                .toArray();
    }

    private Map<String, Schema.ColType> getColTypes() {
        Map<String, Schema.ColType> colTypes = Arrays.stream(conf.getDatasetConfig().getMetricColumns())
                .collect(Collectors.toMap(Function.identity(), c -> Schema.ColType.DOUBLE));

        if (!Strings.isNullOrEmpty(conf.getDatasetConfig().getLabelColumn())) {
            colTypes.put(conf.getDatasetConfig().getLabelColumn(), Schema.ColType.DOUBLE);
        }

        return colTypes;
    }

}
