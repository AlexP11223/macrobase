package alexp.macrobase.pipeline.benchmark;

import alexp.macrobase.evaluation.GridSearch;
import alexp.macrobase.evaluation.memory.BasicMemoryProfiler;
import alexp.macrobase.explanation.Explanation;
import alexp.macrobase.outlier.Trainable;
import alexp.macrobase.outlier.Updatable;
import alexp.macrobase.pipeline.Pipeline;
import alexp.macrobase.pipeline.Pipelines;
import alexp.macrobase.pipeline.benchmark.config.*;
import alexp.macrobase.pipeline.benchmark.result.ExecutionResult;
import alexp.macrobase.pipeline.benchmark.result.ResultFileWriter;
import alexp.macrobase.pipeline.benchmark.result.ResultHolder;
import alexp.macrobase.pipeline.benchmark.result.ResultWriter;
import alexp.macrobase.pipeline.config.StringObjectMap;
import alexp.macrobase.streaming.StreamGenerator;
import alexp.macrobase.streaming.Windows.WindowManager;
import alexp.macrobase.utils.BenchmarkUtils;
import com.google.common.base.Strings;
import com.google.common.collect.Iterables;
import edu.stanford.futuredata.macrobase.analysis.classify.Classifier;
import edu.stanford.futuredata.macrobase.datamodel.DataFrame;
import edu.stanford.futuredata.macrobase.datamodel.Schema;
import org.apache.commons.io.FilenameUtils;

import java.util.*;
import java.util.function.Function;
import java.util.stream.Collectors;

import static alexp.macrobase.utils.BenchmarkUtils.aucCurve;

public class BenchmarkPipeline extends Pipeline {

    private final ExecutionType executionType;

    private final ExecutionConfig conf;
    private final String rootDataDir;
    private ResultWriter resultWriter;
    private final String timeColumn = "__autogenerated_time";
    private DataFrame dataFrame;
    private int[] labels;

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf) {
        this(executionType, conf, null, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir) {
        this(executionType, conf, rootDataDir, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir, ResultWriter resultWriter) {
        this.executionType = executionType;
        this.conf = conf;
        this.rootDataDir = rootDataDir;
        this.resultWriter = resultWriter;
    }

    public void run() throws Exception {
        if (resultWriter == null) {
            setupResultWriter();
        }

        switch (executionType) {
            case BATCH_CLASSIFICATION:
                classificationMode();
                break;
            case STREAMING_CLASSIFICATION:
                streamingMode();
                break;
            case EXPLANATION:
                explanationMode();
                break;
        }
    }

    private void classificationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();

        printInfo(String.format("Running %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);
        StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf);

        if (!algorithmParameters.equals(classifierConf.getParameters())) {
            out.println(algorithmParameters);
        }
        Classifier classifier = Pipelines.getClassifier(
                classifierConf.getAlgorithmId(),
                algorithmParameters,
                conf.getDatasetConfig().getMetricColumns(),
                conf.getDatasetConfig().getDatasetId());
        ResultHolder resultHolder = runClassifier(classifier);

        printInfo(String.format("Training time: %d ms (%.2f sec), Classification time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
                resultHolder.getTrainingTime(), resultHolder.getTrainingTime() / 1000.0,
                resultHolder.getClassificationTime(), resultHolder.getClassificationTime() / 1000.0,
                resultHolder.getMaxMemoryUsage() / 1024 / 1024,
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).rocArea()),
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).prArea())));

        resultWriter.write(resultHolder.getResultsDf(),
                new ExecutionResult(resultHolder.getTrainingTime(), resultHolder.getClassificationTime(),
                        0,
                        resultHolder.getMaxMemoryUsage(), conf, algorithmParameters)
                        .setClassifierId(classifierConf.getAlgorithmId())
        );
    }

    private void streamingMode() throws Exception {
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //
        List<List<Long>> streamTrainTimeReps = new ArrayList<>();
        List<List<Long>> streamPredictTimeReps = new ArrayList<>();
        List<List<Long>> streamUpdateTimeReps = new ArrayList<>();
        List<Long> streamMemoryPeakReps = new ArrayList<>();
        List<DataFrame> streamDFReps = new ArrayList<>();
        List<DataFrame> streamMIReps = new ArrayList<>();
        List<StringObjectMap> streamMPReps = new ArrayList<>();
        List<Double> performanceReps = new ArrayList<>();
        List<double[]> streamScores = new ArrayList<>();
        Classifier clf = null;
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //
        int streaming_reps = 100 ;
        // Repeat the streaming n times (that is because of non-deterministic algorithms)
        for (int rep = 0; rep < streaming_reps; rep++) {
            System.out.println("Streaming mode: ON. REP = " + rep);
            // - - - - - - - - - - - - - - - - - - - - - - - - - - //
            DataFrame streamDF;
            DataFrame streamMI;
            final List<Long> streamTrainTime = new ArrayList<>();
            final List<Long> streamPredictTime = new ArrayList<>();
            final List<Long> streamUpdateTime = new ArrayList<>();
            BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
            StreamGenerator.init();
            String rawDataPoint = "";
            // - - - - - - - - - - - - - - - - - - - - - - - - - - //
            AlgorithmConfig classifierConf = conf.getClassifierConfig();
            printInfo(String.format("Running %s %s on %s", classifierConf.getAlgorithmId(), classifierConf.getParameters(), conf.getDatasetConfig().getUri().getOriginalString()));
            if (resultWriter == null) { // Print the classifier information
                setupResultWriter();
            }
            StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf); // Validate the algorithm parameters
            if (!algorithmParameters.equals(classifierConf.getParameters())) {
                out.println(algorithmParameters);
            }
            WindowManager wm = new WindowManager(classifierConf, conf.getDatasetConfig()); // Initialize window manager
            String windowType = wm.getWindowMethod();
            Classifier streamingClassifier = Pipelines.getClassifier( // Build the Streaming Classifier Model
                    classifierConf.getAlgorithmId(),
                    algorithmParameters,
                    conf.getDatasetConfig().getMetricColumns(),
                    conf.getDatasetConfig().getDatasetId()
            );
            clf = streamingClassifier;
            if (windowType.equals("none")) { // Make sure that the current classifier is streaming classifier.
                return;
            }
            while (true) { // Iteratively Repeat (Streaming Simulation)
                if (!wm.windowIsConstructed()) {

                    rawDataPoint = StreamGenerator.fetch(conf.getDatasetConfig().getUri().getPath()); // Read a raw data point from the Stream Generator

                    wm.manage(rawDataPoint); // Obtain the window when the window method invariants are satisfied

                    if (wm.getWindowSize() <= 0) {
                        break; // Stop streaming simulation when the real size of the window is zero
                    }

                } else {

                    dataFrame = wm.getWindowDF(); // Build the window DataFrame

                    createAutoGeneratedColumns(dataFrame, timeColumn); // Add a time column to the DataFrame

                    streamTrainTime.add(streamingClassifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
                        ((Trainable) streamingClassifier).train(dataFrame);
                    }) : 0);

                    streamPredictTime.add(BenchmarkUtils.measureTime(() -> {
                        streamingClassifier.process(dataFrame);
                    }));

                    streamUpdateTime.add(streamingClassifier instanceof Updatable ? BenchmarkUtils.measureTime(() -> {
                        ((Updatable) streamingClassifier).update(dataFrame);
                    }) : 0);

                    wm.clearWindowData();  // Clear the window data (in order to continue to the next window construction)

                    if (wm.isEndStream()) {
                        break; // Stop streaming simulation when the generator is empty
                    }

                }
            } // End of streaming simulation
            long streamMemoryPeak = memoryProfiler.getPeakUsage();
            streamDF = streamingClassifier.getResults();
            streamMI = streamingClassifier.getModelInfo();
            if (streamDF != null) {
                double[] scores = streamDF.getDoubleColumnByName(streamingClassifier.getOutputColumnName());
                double performance = aucCurve(scores, getLabels(streamDF)).rocArea();
                streamScores.add(scores);
                streamTrainTimeReps.add(streamTrainTime);
                streamPredictTimeReps.add(streamPredictTime);
                streamUpdateTimeReps.add(streamUpdateTime);
                streamMemoryPeakReps.add(streamMemoryPeak);
                streamDFReps.add(streamDF);
                streamMIReps.add(streamMI);
                streamMPReps.add(algorithmParameters);
                performanceReps.add(performance);
            } else {
                break;
            }
            // Continue to the next rep..
        }
        // Repetitions has been completed.
        if (!streamDFReps.isEmpty()) {
            // Calculate the average scores
            double[] scoresAVG = new double[streamScores.get(0).length];
            for (int col = 0; col < streamScores.get(0).length; col++) {
                double colSum = 0;
                for (double[] streamScore : streamScores) {
                    colSum += streamScore[col];
                }
                scoresAVG[col] = colSum / streaming_reps;
            }

            // Calculate the average performance
            double performanceAVG = aucCurve(scoresAVG, getLabels(streamDFReps.get(0))).rocArea();
            // Find the index of the model that its performance is closer to the average performance
            double[] performanceDistances = new double[performanceReps.size()];
            for (int idx = 0; idx < performanceReps.size(); idx++) {
                performanceDistances[idx] = Math.abs(performanceAVG - performanceReps.get(idx));
            }
            int avgModelIndex = 0;
            double minValue = performanceDistances[avgModelIndex];
            for (int idx = 0; idx < performanceDistances.length; idx++) {
                if (minValue > performanceDistances[idx]) {
                    minValue = performanceDistances[idx];
                    avgModelIndex = idx;
                }
            }
            // Find the information of the average model
            double modelPerformance = performanceReps.get(avgModelIndex);
            long modelTTime = avg(streamTrainTimeReps.get(avgModelIndex));
            long modelPTime = avg(streamPredictTimeReps.get(avgModelIndex));
            long modelUTime = avg(streamUpdateTimeReps.get(avgModelIndex));
            long modelMPeak = streamMemoryPeakReps.get(avgModelIndex);
            DataFrame model = streamDFReps.get(avgModelIndex);
            DataFrame modelInfo = streamMIReps.get(avgModelIndex);
            StringObjectMap modelParams = streamMPReps.get(avgModelIndex);

            // Write the results of the average model
            resultWriter.write(model, new ExecutionResult(modelTTime, modelPTime, modelUTime, modelMPeak, conf, modelParams));
            // Write the model info
            if (modelInfo != null) {
                clf.setModelInfo(modelInfo);
            }
            // Print results
            printInfo(String.format("" +
                            "Training time: %f sec, " +
                            "Classification time: %f sec, " +
                            "Update time: %f sec, " +
                            "Max memory usage: %d MB, " +
                            "ROC AUC: %.4f",
                    ((double) modelTTime / 1000.0),
                    ((double) modelPTime / 1000.0),
                    ((double) modelUTime / 1000.0),
                    (modelMPeak / 1024 / 1024),
                    (modelPerformance)
            ));
        } else {
            System.out.println("[Alert] There are no data points processed by the current algorithm");
        }
        // End of streaming mode
    }

    private static long avg(List<Long> list) {
        long sum = 0;
        for (long i : list) {
            sum += i;
        }
        return sum / list.size();
    }

    private void explanationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();
        AlgorithmConfig explainerConf = conf.getExplainerConfig();

        printInfo(String.format("Running Classifier %s %s + Explainer %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                explainerConf.getAlgorithmId(), explainerConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);

        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();

        Explanation explainer = Pipelines.getExplainer(explainerConf, classifierConf, conf.getDatasetConfig(), conf.getSettingsConfig().getExplanationSettings());
        final long explanationTime = BenchmarkUtils.measureTime(() -> {
            explainer.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        System.out.println("\nTime " + explanationTime / 1000.0 + " sec");
//                printInfo(String.format("Explanation time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
//                        explanationTime, explanationTime / 1000.0,
//                        maxMemoryUsage / 1024 / 1024,
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).rocArea()),
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).prArea())));
        resultWriter.write(explainer.getResults(), new ExecutionResult(0, explanationTime, 0, maxMemoryUsage,
                conf, explainerConf.getParameters())
                .setClassifierId(classifierConf.getAlgorithmId())
                .setExplainerId(explainerConf.getAlgorithmId()));
    }

    private ResultHolder runClassifier(Classifier classifier) throws Exception {
        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
        final long trainingTime = classifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
            ((Trainable) classifier).train(dataFrame);
        }) : 0;
        final long classificationTime = BenchmarkUtils.measureTime(() -> {
            classifier.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        DataFrame resultsDf = classifier.getResults();
        return new ResultHolder(resultsDf, trainingTime, classificationTime, maxMemoryUsage);
    }

    private StringObjectMap getAlgorithmParameters(AlgorithmConfig algorithmConfig) throws Exception {
        StringObjectMap baseParams = algorithmConfig.getParameters();
        GridSearchConfig gridSearchConfig = algorithmConfig.getGridSearchConfig();
        if (gridSearchConfig == null) {
            return baseParams;
        }
        out.println(String.format("Running Grid Search, using %s measure", gridSearchConfig.getMeasure().toUpperCase()));
        GridSearch gs = new GridSearch();
        gs.addParams(gridSearchConfig.getParameters());
        gs.setOutputStream(out);
        gs.run(params -> {
            StringObjectMap currentParams = baseParams.merge(params);
            Classifier classifier = Pipelines.getClassifier(
                    algorithmConfig.getAlgorithmId(),
                    currentParams,
                    conf.getDatasetConfig().getMetricColumns(),
                    conf.getDatasetConfig().getDatasetId());
            classifier.process(dataFrame);
            DataFrame classifierResultDf = classifier.getResults();
            double[] classifierResult = classifierResultDf.getDoubleColumnByName(classifier.getOutputColumnName());
            switch (gridSearchConfig.getMeasure()) {
                case "roc":
                    return aucCurve(classifierResult, labels).rocArea();
                case "pr":
                    return aucCurve(classifierResult, labels).prArea();
                default:
                    throw new RuntimeException("Unknown search measure " + gridSearchConfig.getMeasure());
            }
        });
        SortedMap<Double, Map<String, Object>> gsResults = gs.getResults();
        return baseParams.merge(new StringObjectMap(Iterables.getLast(gsResults.values())));
    }

    private void setupResultWriter() {
        resultWriter = new ResultFileWriter(executionType)
                .setOutputDir(getOutputDir())
                .setBaseFileName(FilenameUtils.removeExtension(conf.getDatasetConfig().getDatasetId())); // doesn't matter, currently always initialized in constructor
    }

    private DataFrame loadData() throws Exception {
        Map<String, Schema.ColType> colTypes = getColTypes();

        List<String> requiredColumns = new ArrayList<>(colTypes.keySet());

        DataFrame dataFrame = Pipelines.loadDataFrame(conf.getDatasetConfig().getUri().addRootPath(rootDataDir), colTypes, requiredColumns, conf.getDatasetConfig().toMap());

        createAutoGeneratedColumns(dataFrame, timeColumn); // needed for MCOD

        return dataFrame;
    }

    private int[] getLabels(DataFrame dataFrame) {
        String labelColumn = conf.getDatasetConfig().getLabelColumn();
        if (Strings.isNullOrEmpty(labelColumn)) {
            return null;
        }

        return Arrays.stream(dataFrame.getDoubleColumnByName(labelColumn))
                .mapToInt(d -> (int) d)
                .toArray();
    }

    private Map<String, Schema.ColType> getColTypes() {
        Map<String, Schema.ColType> colTypes = Arrays.stream(conf.getDatasetConfig().getMetricColumns())
                .collect(Collectors.toMap(Function.identity(), c -> Schema.ColType.DOUBLE));

        if (!Strings.isNullOrEmpty(conf.getDatasetConfig().getLabelColumn())) {
            colTypes.put(conf.getDatasetConfig().getLabelColumn(), Schema.ColType.DOUBLE);
        }

        return colTypes;
    }

}
